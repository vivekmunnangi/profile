<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Portfolio</title>
<style>
body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin:0; padding:0; background:#f0f4f8; color:#333; }
header {
  display:flex; align-items:center; padding:1rem 2rem; background:#fff; border-bottom:2px solid #ccc;
  position: fixed; top:0; left:0; width:100%; z-index:1100;
}
header img { height:60px; margin-right:20px; }
h1,h2,h3 { color:#0055a5; }
h2 { border-bottom:2px solid #0055a5; padding-bottom:0.3rem; margin-bottom:1rem; }
section { margin-bottom:3rem; }

.container { display: flex; margin-top:80px; } /* leave space for fixed header */
.sidebar {
  width: 220px;
  position: fixed;
  top: 80px; /* below header */
  left: 0;
  height: calc(100vh - 80px);
  padding: 2rem 1rem;
  background: #f0f4f8;
  box-shadow: 2px 0 5px rgba(0,0,0,0.1);
  display: flex;
  flex-direction: column;
  align-items: center;
}
.profile-pic {
  width: 100%;
  border-radius: 8px;
  margin-bottom: 1rem;
}
.education {
  font-style: italic;
  font-size: 0.9rem;
  color: #333;
  text-align: center;
  margin: 0.2rem 0;
}
.content {
  margin-left: 250px; /* space for sidebar */
  padding: 2rem;
  max-width: 1000px;
  margin-right: auto;
}

/* Experience Grid */
.experience-grid {
  display: grid;
  gap: 1.5rem;
}
@media (max-width: 600px) {
  .experience-grid { grid-template-columns: 1fr; }
}
@media (min-width: 601px) and (max-width: 1000px) {
  .experience-grid { grid-template-columns: repeat(2, 1fr); }
}
@media (min-width: 1001px) {
  .experience-grid { grid-template-columns: repeat(3, 1fr); }
}
.experience-box {
    background:#fff; padding:1rem; border-left:5px solid #0055a5; box-shadow:0 0 10px rgba(0,0,0,0.05);
    cursor:pointer; display:flex; flex-direction:column; justify-content:space-between; aspect-ratio:1/1; position:relative;
}
.experience-box h3 { margin-top:0; text-align:center; }
.experience-box p { text-align:center; margin-top:0.5rem; }
.experience-box .full-content { display:none; }
.slideshow-preview { position:relative; width:100%; flex:1; margin:0.5rem 0; overflow:hidden; }
.slideshow-preview img {
    position:absolute; top:0; left:0; width:100%; height:100%; object-fit:cover; opacity:0; transition:opacity 1s;
}
.slideshow-preview img.active { opacity:1; }
a { color:#0055a5; text-decoration:none; }
a:hover { text-decoration:underline; }
.modal { display:none; position:fixed; top:0; left:0; width:100%; height:100%; background:rgba(0,0,0,0.7); justify-content:center; align-items:center; z-index:1000; }
.modal-content { background:#fff; padding:2rem; max-width:800px; width:90%; max-height:90%; overflow-y:auto; position:relative; border-radius:8px; }
.modal-close { position:absolute; top:1rem; right:1rem; font-size:1.5rem; cursor:pointer; color:#0055a5; }
.image-row { display:flex; gap:1rem; flex-wrap:wrap; justify-content:center; margin-bottom:1rem; }
.image-row figure { flex:1 1 200px; text-align:center; margin:0; }
.image-row img { width:100%; height:auto; object-fit:contain; }

/* Mobile behavior */
@media (max-width: 768px) {
  .sidebar {
    position: relative;
    top: 0;
    width: 100%;
    height: auto;
    box-shadow: none;
    margin-bottom: 1rem;
  }
  .content {
    margin-left: 0;
    padding: 1rem;
  }
}
</style>
</head>
<body>
<header>
    <img src="{ logo_url }" alt="Company Logo"/>
    <h1>Career</h1>
</header>

<div class="container">
    <!-- Fixed left sidebar -->
    <aside class="sidebar">
        <img src="../images/my_picture.jpg" alt="Vivek Reddy Munnangi" class="profile-pic">
        <p class="education">Masters in Data Science, IU Luddy</p>
        <p class="education">Bachelors in Mechanical Engineering</p>
    </aside>

    <!-- Main scrolling content -->
    <main class="content">
        <section><h2>Company Description</h2><p>Google’s mission is to organize the world’s information and make it universally accessible and useful. Founded in 1998 by Larry Page and Sergey Brin, Google has grown from a dorm-room project into a global technology leader, creating products that serve billions of users daily, including Google Search, YouTube, Android, Gmail, and Google Cloud. At its core, Google leverages cutting-edge technology, artificial intelligence, and machine learning to solve complex problems, enhance user experiences, and enable meaningful connections worldwide. Beyond products, Google drives innovation through AI research, exploring applications in natural language processing, computer vision, generative AI, and large-scale data systems. The company prioritizes accessibility, fairness, and inclusivity in all its technological solutions, ensuring that its impact benefits communities globally while maintaining a culture of collaboration, creativity, and ethical innovation.</p></section>
        <section><h2>Scope</h2><p>At Google, the scope of work is broad and impactful, spanning multiple domains including AI, ML, cloud computing, computer vision, natural language processing, and generative AI. Engineers contribute to projects that operate at massive scale, influencing billions of users globally. Whether implementing computer vision solutions for YouTube, developing LLM-based tools for Search, or creating multi-modal generative AI models for Google Cloud, the scope involves both deep technical challenges and strategic innovation. Projects require designing scalable infrastructure, optimizing model performance, processing vast datasets, and building systems that are reliable, efficient, and secure. The work scope is not limited to individual contributions—engineers collaborate across teams, bridging research and production, while maintaining a focus on solving real-world problems that enhance accessibility, usability, and fairness for end-users everywhere.</p></section>
        <section><h2>Work Culture</h2><p>Google fosters a unique, collaborative, and innovative work culture. Creativity, curiosity, and experimentation are highly encouraged, allowing teams to explore unconventional solutions while adhering to best practices. The company values diversity and inclusivity, emphasizing that teams should reflect the communities their products serve. Cross-functional collaboration is common, and employees are empowered to contribute ideas, challenge assumptions, and participate in decision-making processes. Google also prioritizes learning and professional growth, providing access to state-of-the-art technologies, mentorship programs, and opportunities to rotate across teams. Work-life balance, employee well-being, and a supportive environment contribute to a culture where innovation thrives and everyone can meaningfully impact technology for billions of users.</p></section>
        <section><h2>Motivation to Join KNAPP</h2><p>I am motivated to work at Google because of its unparalleled commitment to leveraging technology to solve complex, real-world problems at scale. The opportunity to work on projects involving AI, ML, computer vision, and generative AI, while directly impacting billions of users, aligns perfectly with my passion for applying technical knowledge to meaningful challenges. Google’s focus on fairness, accessibility, and inclusivity resonates with my values, and the company’s culture of innovation, collaboration, and continuous learning inspires me to contribute my best. Being part of an organization that encourages experimentation, embraces emerging technologies, and pushes the boundaries of what’s possible motivates me to grow as an engineer while contributing to transformative solutions.</p></section>
        <section><h2>Why I am a Great Fit</h2><p>I am a strong fit for this role because my academic and professional background aligns closely with Google’s technical and cultural requirements. I have experience in AI, ML, computer vision, deep learning, and large-scale data processing, with hands-on expertise in Python, C++, R, SQL, ROS2, TensorFlow, PyTorch, and relevant frameworks. My work on autonomous vehicles, medical image registration, and real-time API systems demonstrates my ability to design, optimize, and deploy complex software solutions. I am a fast learner, capable of adapting to new technologies and concepts, and I thrive in collaborative, cross-functional environments. My problem-solving skills, combined with strong teamwork, leadership experience, and a commitment to delivering high-quality results, make me well-prepared to contribute effectively to Google’s projects and culture.</p></section>
        <section><h2>Work Experience</h2>
            <div class="experience-grid">
<div class="experience-box" data-title="Research Engineer, VAIL IU">
    <h3>Research Engineer, VAIL IU</h3>
    <div class="slideshow-preview">
        <img src="../images/s_withcar.jpg" alt="Vehicle Model"/>
        <img src="../images/group_withcar.jpg" alt="Team"/>
        <img src="../images/grip_analysis.png" alt="Dashboard"/>
    </div>
    <p>State estimation, vehicle dynamics, and control development for IU Luddy Racing’s autonomous Indy Autonomous Challenge car.</p>
    <div class="full-content">
        <h3>Research Engineer, Vehicle Autonomy and Intelligence Lab – IU Luddy Racing</h3>
        <div class="image-row">
            <figure>
                <img src="../images/s_withcar.jpg" alt="Vehicle Model"/>
                <figcaption>Dallara AV-24 – IU Luddy Racing Racecar</figcaption>
            </figure>
            <figure>
                <img src="../images/group_withcar.jpg" alt="Team"/>
                <figcaption>IU Luddy Racing Team – Laguna Seca Race Weekend</figcaption>
            </figure>
        </div>

        <p>
            I joined the team in <strong>August 2024</strong>, about seven months into development, shortly after
            IU Luddy Racing announced its participation in the <strong>Indy Autonomous Challenge (IAC)</strong>.
            My focus was on <strong>state estimation and vehicle dynamics</strong>, working alongside the
            controller and localization leads to solve state-related problems and refine the dynamics model.
        </p>

        <p>
            After onboarding, I identified the need for a more advanced dynamics model and proposed a
            <strong>grip prediction module</strong> to estimate traction limits. This allowed us to design
            safer, more aggressive <strong>velocity profiles</strong> using offline planners. I also analyzed
            <strong>engine performance data</strong>, building strategies for <strong>optimal gear shifting</strong>
            based on ECU and dynamometer data to adapt power delivery to track demands.
        </p>

        <p>
            Our first dynamic module control tests took place at <strong>Las Vegas Motor Speedway (LVMS) in January 2025</strong>,
            showing promising results. Over the next months, we prepared for <strong>multi-car racing</strong>, updating the
            stack for robustness and safety. In <strong>July 2025</strong>, we competed at <strong>Laguna Seca Raceway</strong>
            against eight international teams. Despite a late practice crash that forced speed adjustments,
            we achieved <strong>4th place overall</strong> – a strong showing for our first-ever road course race
            and only our third competition as a new team.
        </p>

        <div class="image-row">
            <figure>
                <img src="../images/grip_analysis.png" alt="Dashboard"/>
                <figcaption>Grip Prediction and Race Data Analysis</figcaption>
            </figure>
        </div>

        <p>
            These milestones gave us valuable data on vehicle dynamics, localization, and controller behavior.
            Going forward, I continue developing new modules to push performance and safety, helping
            IU Luddy Racing compete at the cutting edge of autonomous motorsport.
        </p>

        <p><a href="https://vail-robotics.net/pages/people#" target="_blank">My Research Profile</a></p>
    </div>
</div>


<div class="experience-box" data-title="Partner & AI Head, DentalMatrix.ai">
    <h3>AI Head, DentalMatrix.ai</h3>
    <div class="slideshow-preview">
        <img src="../images/writeoff_description.png" alt="Dashboard"/>
        <img src="../images/writeoff_distribution.png" alt="Distribution"/>
    </div>
    <p>Co-founded and developed a unified dental platform with real-time ETL pipelines and a fine-tuned AI agent for actionable practice insights.</p>

    <div class="full-content">
        <h3>AI Head, DentalMatrix.ai</h3>

        <p>
            At DentalMatrix.ai, we built a platform to help dental clinics unify their patient and lead data. 
            Clinics often manage patient records in OpenDental and track new inquiries in GoHighLevel CRM. 
            Our solution integrates these systems into a single database, allowing clinics to track existing patients 
            and potential leads in one place. This unified data enables clinics to target marketing campaigns, convert 
            potential patients into real appointments, and improve overall operational efficiency. So with this I started working on sync operations using API on <strong>September 2024</strong>
        </p>

        <p>
            We also developed a domain-specific <strong>AI Agent</strong> to interact with clinic staff. Unlike generic 
            chatbots, this agent is fine-tuned on dental terminology, CRM data structures, and official dental abbreviations. 
            It provides answers to natural-language questions with either <strong>textual explanations or data visualizations</strong>. 
            Examples of queries include: 
            <em>"Which procedures have the highest write-offs?"</em> or 
            <em>"Show conversion rates for new patient inquiries this month."</em>
        </p>

        <p>
            The AI Agent uses a <strong>Qwen-3 base model</strong> for its multilingual capabilities, 
            allowing clinics to translate campaigns and communications into languages like Spanish automatically. 
            Fine-tuning was performed on <strong>2× H100 GPUs</strong> using <strong>DeepSpeed, Fully Sharded Data Parallel (FSDP), and parallel computing</strong> 
            to accelerate training and optimize performance.
        </p>

        <p>
            The backend includes a robust <strong>ETL pipeline</strong> that extracts and synchronizes structured data 
            from OpenDental and the CRM into a centralized relational database. This enables real-time CRUD operations 
            and ensures all patient and lead information is immediately up-to-date.
        </p>
        <p> When asked about <em>"Which procedures have the highest write-offs?"</em>, the agent reaponse as text was <em> "The WriteOff is calculated as: (Sum of all fees on procedures - Sum of all insurance estimates and insurance payments received) + WriteoffsAlreadySent.  It's not just a simple sum of all writeoffs.  The user can never directly edit this field, but it is possible to set it blank.  If the WriteOff value is higher than the WriteOffEst, then we show the WriteOff value in color red.  This means that it has been manually altered from the estimate. We don't currently track who changed it."</em>, and the visual response as shown below </p>
        <div class="image-row">
            <figure>
                <img src="../images/writeoff_description.png" alt="Dashboard"/>
                <figcaption>AI Agent visualization: Patient write-off insights</figcaption>
            </figure>
            <figure>
                <img src="../images/writeoff_distribution.png" alt="Distribution"/>
                <figcaption>AI Agent visualization: Distribution of total write-offs amount</figcaption>
            </figure>
        </div>

        <p>
            This platform provides dental clinics with a unified view of all contacts, actionable insights for resource planning, 
            marketing, and patient management, and an intelligent assistant designed specifically for dental operations. 
            With this foundation, clinics can better understand patient needs, optimize marketing efforts, 
            and deliver higher quality care efficiently.
        </p>
        <p><a href="https://dentalmetrics.ai/" target="_blank">Link to site(Under development)</a></p>
    </div>
</div>


<div class="experience-box" data-title="Research Assistant">
    <h3>Research Assistant, Frontiers of Optical Imaging and Biology Lab</h3>
    
    <div class="slideshow-preview">
        <img src="../images/lab_logo.png" alt="Lab Logo"/>
        <img src="../images/Mice_Eye_3D.png" alt="3D Eye Scan"/>
        <img src="../images/is_after.png" alt="Registered Eye Scan"/>
    </div>
    
    <p>Developed a dynamic 3D image registration pipeline to correct cellular-level motion in high-resolution eye scans across multiple subjects.</p>
    
    <div class="full-content">
        <h3>Research Assistant, Frontiers of Optical Imaging and Biology Lab</h3>
        <p>
            In this research, I worked on aligning 3D eye scan datasets from humans, mice, and bovine subjects. 
            Each scan captures cellular structures at <strong>2 µm focus</strong>, where even small biological motions 
            or scanner delays cause misalignment. The goal was to dynamically correct drift while preserving the natural movement of cells.
        </p>
        
        <p>
            The workflow included:
            <ul>
                <li><strong>Pixel classification:</strong> Identified cell bodies, nuclei, and background pixels using intensity distribution and pooling. Noise pixels were removed.</li>
                <li><strong>Dynamic registration:</strong> Sequentially aligned frames by tracking drift and adjusting each frame relative to the first, while allowing live cells to move naturally.</li>
                <li><strong>Subject-agnostic design:</strong> The code works across different species and datasets without manual tuning.</li>
            </ul>
        </p>
        <div class="image-row">
            <figure>
                <img src="../images/sv_before_drift.png" alt="Side view before"/>
                <figcaption>Side view before registration</figcaption>
            </figure>
            <figure>
                <img src="../images/sv_after_drift.png" alt="Side view after"/>
                <figcaption>Side view after registration</figcaption>
            </figure>
        </div>

        <div class="image-row">
            <figure>
                <img src="../images/sv_before.png" alt="Front view before"/>
                <figcaption>Front view before registration</figcaption>
            </figure>
            <figure>
                <img src="../images/sv_after.png" alt="Front view after"/>
                <figcaption>Front view after registration</figcaption>
            </figure>
        </div>

        <p>
            The registration consistently produced highly quality alignment with 97% and at  6.5 MB/s processing speed, making cellular structures clearly visible 
            in top, front, and side views, and enabling downstream analysis of cell motion and behavior.
        </p>
        

        <div class="image-row">
            <figure>
                <img src="../images/is_before.png" alt="Top view before"/>
                <figcaption>Top view before registration (3D stack)</figcaption>
            </figure>
            <figure>
                <img src="../images/is_after.png" alt="Top view after"/>
                <figcaption>Top view after registration (3D stack)</figcaption>
            </figure>
        </div>
        <div class="image-row">
            <figure>
                <img src="../images/Mice_Eye_3D.png" alt="Top view before"/>
                <figcaption>Mice Eye constructed as 3D image from scan</figcaption>
            </figure>
        </div>
        <p><a href="https://blogs.iu.edu/tankamlab/people/" target="_blank">My Research Profile</a></p>
    </div>
</div>

<div class="experience-box" data-title="Consultant, EDD chat assistant">
    <h3>Consultant, EDD chat assistant</h3>

    <div class="slideshow-preview">
        <img src="../images/edd_logo.png" alt="CEDS Dashboard"/>
        <img src="../images/ceds_template.png" alt="AI Integration"/>
        <img src="../images/edd_region.png" alt="AI Integration"/>
    </div>

    <p>Developed an AI-driven tool to assist Economic Development Districts (EDDs) in creating Comprehensive Economic Development Strategies (CEDS).</p>

    <div class="full-content">
        <h3>Consultant, EDD chat assistant</h3>

        <p>
            From August 2024 to January 2025, I worked on integrating AI into the **CEDS creation process** for under-resourced Economic Development Districts (EDDs). 
            The CEDS is a strategic plan used to drive regional economic growth and is critical for securing federal funding through the Economic Development Administration (EDA). 
            Many EDDs face challenges in creating strong CEDS documents due to limited resources, expertise, and access to data.
        </p>
        <div class="image-row">
            <figure>
                <img src="../images/edd_region.png" alt="Top view before"/>
                <figcaption>Interactive map of Economic Development District - EDD Zones</figcaption>
            </figure>

        <p>
            <strong>Problem:</strong> Creating a robust CEDS is time-intensive and requires data analysis, stakeholder engagement, and alignment with EDA guidelines. Smaller districts often struggle to meet these requirements, delaying funding and limiting economic development opportunities.
        </p>

        <p>
            <strong>Solution:</strong> The project aimed to develop an AI-driven CEDS generator that leverages generative AI and transformer-based models to automate key parts of the process. Key features include:
            <ul>
                <li><strong>Structured Input Interface:</strong> Predefined fields for entering regional data to ensure alignment with EDA guidelines.</li>
                <li><strong>LLM Integration:</strong> Large language models process inputs and generate region-specific strategies using historical CEDS data.</li>
                <li><strong>SWOT Automation:</strong> AI-assisted SWOT analyses and strategic recommendations based on regional data.</li>
                <li><strong>Evaluation Framework Alignment:</strong> Automated feedback to ensure generated CEDS meets EDA evaluation criteria.</li>
                <li><strong>Scalability:</strong> Modular design allowing use across multiple EDDs with regional customization.</li>
            </ul>
        </p>
        <div class="image-row">
            <figure>
                <img src="../images/ceds_template.png" alt="Top view before"/>
                <figcaption>Basic CEDS Template</figcaption>
            </figure>
            <figure>
                <img src="../images/inital_proposal.png" alt="Top view after"/>
                <figcaption>Initial Proposal</figcaption>
            </figure>
        </div>
        <p>
            <strong>Implementation Strategy:</strong> 
            - Built a prototype integrating AI( used VLLM mistral architecture with mutiple Lora adaptors targeting different layers of base model) with SWOT analysis and draft CEDS generation.  
            - Used cosine similarity to compare generated content against approved CEDS documents, focusing on areas like climate resilience and equity.  
            - Incorporated feedback loops for planners to iteratively refine inputs and outputs.  
            - Designed front-end mockups and modular agent frameworks for user-friendly interaction.
        </p>

        <p>
            <strong>Impact:</strong> This project demonstrated the potential for AI to streamline CEDS creation, automate labor-intensive tasks, and improve document quality and compliance. The prototype empowered planners to create data-backed, EDA-compliant strategies more efficiently.  
            While I paused working on this project after January 2025 due to time constraints, the foundation provides a scalable framework for future AI-assisted economic development tools.
        </p>

        <div class="image-row">
            <figure>
                <img src="../images/pn_analysis.png" alt="CEDS Dashboard"/>
                <figcaption>Positive and negative statment analysis per topics</figcaption>
            </figure>


        </div>
    </div>
</div>


</div>
        </section>
        <section><h2>Impact Projects</h2>
            <div class="experience-grid">
<div class="experience-box" data-title="Full-Stack Web Application for Restaurants">
    <h3>Full-Stack Web Application</h3>
    <div class="slideshow-preview">
        
        <img src="../images/adt_logo2.png" alt="Customer Data Interface"/>
    </div>
    <p>Developed a full-stack web application using the MERN stack to manage customer preferences, allergies, and reservations efficiently.</p>
    <div class="full-content">
        <h3>Full-Stack Web Application for Restaurants | MERN Stack, Render</h3>
        <p>
            The goal was to create a comprehensive web application for restaurant owners and staff to store, retrieve, and manage customer details effectively.
        </p>
        <p>
            This system centralizes key customer information, including food preferences, allergies, and special preparation instructions, ensuring both safety and personalized service. 
            By integrating a user friendly interface, restaurant staff can efficiently handle reservations, walk-ins, and customer updates with minimal errors.
        </p>
        <p>
            Technically, the application leverages the MERN stack (MongoDB, Express, React, Node.js) for full stack functionality, combined with Render for hosting. 
            For advanced data processing and scalability, Databricks was used to enable large-scale machine learning workflows and optimized database queries.
        </p>
        <p>
            <strong>Impact:</strong> This solution enhances restaurant operations by providing a reliable, intuitive system for managing customer data. 
            It improves the overall dining experience by allowing staff to cater to individual preferences, manage allergies safely, and streamline food preparation workflows. 
            Compared to existing systems, our application offers better integration, usability, and customization for the hospitality industry.
        </p>
        <div class="image-row">
            <figure>
                <img src="../images/adt_loginpage.png" alt="Top view before"/>
                <figcaption>Login page of the App</figcaption>
            </figure>
            <figure>
                <img src="../images/retreival_page.png" alt="Top view after"/>
                <figcaption>Retrieving customer info from database using mobile number</figcaption>
            </figure>
        </div>

    </div>
</div>


<div class="experience-box" data-title="Analyzing and Visualizing Usage for Common Coordinate Framework UIs">
    <h3>Clicks Usage Analysis</h3>
    <div class="slideshow-preview">
        <img src="../images/cns_logo.png" alt="CCF UI Analytics Overview"/>
        
    </div>
    <p>Analyzed and visualized user interaction data for the CCF's User Interfaces using Google Analytics</p>
    <div class="full-content">
        <h3>Analyzing and Visualizing Usage for Common Coordinate Framework User Interfaces | Google Analytics</h3>
        <p>
            This project focused on understanding how users interact with the Registration User Interface (RUI) and Exploration User Interface (EUI) of the Common Coordinate Framework (CCF), developed by Indiana University's Cyberinfrastructure for Network Science Center. 
            By leveraging Google Analytics data, we examined user behavior patterns, element usage frequency, and engagement with spatial search and opacity features.
        </p>
        <p>
            The project involved cleaning and preprocessing a dataset of <strong>192,271 user events</strong>, including event timestamps, page locations, and user identifiers. Critical steps included converting Unix epoch timestamps to human-readable formats, handling missing data, and ensuring data consistency for accurate analysis.
        </p>
        <p>
            Our objectives were to:
            <ul>
                <li>Analyze frequency distributions of user events, such as clicks, mouse movements, and interactions with UI elements.</li>
                <li>Identify the most and least used interface components to understand user engagement.</li>
                <li>Quantify the frequency of opacity changes in the RUI and assess spatial search usage in the EUI.</li>
                <li>Provide insights to guide interface improvements and feature prioritization.</li>
            </ul>
        </p>
        <p>
            Visualization techniques included time series analysis to capture engagement trends, bar and pie charts to illustrate UI element usage, and histograms for opacity and spatial search events. Key insights revealed:
            <ul>
                <li>Fluctuating engagement over time, highlighting peak usage periods and optimal times for feature rollouts.</li>
                <li>High interaction with central UI elements, with underutilized features identified for refinement.</li>
                <li>Specific adoption patterns of spatial search, suggesting areas for personalized feature enhancements.</li>
                <li>Opacity adjustments in the RUI indicating user interest in visual customization, guiding future UI enhancements.</li>
            </ul>
        </p>
        <p>
            During validation, we addressed challenges such as data complexity, event identification, missing values, and visualization clarity by:
            <ul>
                <li>Implementing sophisticated data filtering and segmentation.</li>
                <li>Standardizing timestamps and handling missing values through imputation or exclusion.</li>
                <li>Creating focused, user-centric visualizations for each analytical question.</li>
                <li>Categorizing user events with refined approaches to highlight meaningful interactions.</li>
            </ul>
        </p>
        <div class="image-row">
            <figure>
                <img src="../images/fq_uielement.png" alt="Top view before"/>
                <figcaption>Frequency of UI Elements</figcaption>
            </figure>
            <figure>
                <img src="../images/dashboard_vis.png" alt="Top view after"/>
                <figcaption>Dashboard to Visualize Google Analytics Data</figcaption>
            </figure>
        </div>
        <p>
            This work emphasizes the importance of continuous monitoring and data-driven design for improving UI usability. Insights from this analysis can inform longitudinal studies, user segmentation, interface personalization, and potential automated feedback loops, contributing to more responsive and user-friendly CCF interfaces.
        </p>
        <p><strong>Acknowledgments:</strong> Special thanks to Andreas Bueckle and Michael Patrick Ginda for guidance, and to all Indiana University Bloomington Visual Analytics course staff for their support.</p>
    </div>
</div>


</div>
        </section>
    </main>
</div>

<div class="modal" id="modal">
    <div class="modal-content" id="modal-content">
        <span class="modal-close" id="modal-close">&times;</span>
        <div id="modal-inner"></div>
    </div>
</div>

<script>
const boxes = document.querySelectorAll('.experience-box');
const modal = document.getElementById('modal');
const modalInner = document.getElementById('modal-inner');
const modalClose = document.getElementById('modal-close');

// Small preview slideshow
boxes.forEach(box => {
    const previewImgs = box.querySelectorAll('.slideshow-preview img');
    if(previewImgs.length > 0){
        let current = 0;
        previewImgs[current].classList.add('active');
        setInterval(()=>{
            previewImgs[current].classList.remove('active');
            current = (current+1) % previewImgs.length;
            previewImgs[current].classList.add('active');
        },3000);
    }
});

// Modal logic
boxes.forEach(box => {
    box.addEventListener('click', () => {
        const content = box.querySelector('.full-content');
        if(content) {
            modalInner.innerHTML = content.innerHTML;
            modal.style.display = 'flex';
        }
    });
});

modalClose.addEventListener('click', () => { modal.style.display = 'none'; });
window.addEventListener('click', (e) => { if(e.target == modal) modal.style.display = 'none'; });
</script>

<footer style="text-align:center; margin:2rem 0; font-style:italic; font-size:1rem; color:#333;">
    Vivek Reddy Munnangi | 
    <a href="../resume.pdf" target="_blank" style="color:#0055a5; text-decoration:none;">
        Resume
    </a>
</footer>
</body>
</html>
